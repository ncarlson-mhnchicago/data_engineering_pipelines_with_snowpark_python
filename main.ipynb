{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "notebookId": "6zsjpoudz3ife4ks5bmz",
   "authorId": "4427789458918",
   "authorName": "NCARLSON",
   "authorEmail": "ncarlson@mhnchicago.org",
   "sessionId": "cee9ec25-3b23-4b0f-9bfc-88a87442f529",
   "lastEditTime": 1750980721485
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152b9c5c-1d6a-4792-9f21-cb198d9554cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "description"
   },
   "source": [
    "### Data Engineering Pipelines with Snowpark Python\n",
    "\n",
    "What You'll Learn\n",
    "You will learn about the following Snowflake features during this Quickstart:\n",
    "\n",
    "Snowflake's Table Format\n",
    "Data ingestion with COPY\n",
    "Schema inference\n",
    "Data sharing/marketplace (instead of ETL)\n",
    "Streams for incremental processing (CDC)\n",
    "Streams on views\n",
    "Python UDFs (with third-party packages)\n",
    "Python Stored Procedures\n",
    "Snowpark DataFrame API\n",
    "Snowpark Python programmability\n",
    "Warehouse elasticity (dynamic scaling)\n",
    "Visual Studio Code Snowflake native extension (PuPr, Git integration)\n",
    "SnowCLI (PuPr)\n",
    "Tasks (with Stream triggers)\n",
    "Task Observability\n",
    "GitHub Actions (CI/CD) integration\n",
    "\n",
    "Source Data:\n",
    "'s3://sfquickstarts/data-engineering-with-snowpark-python/pos';\n",
    "'s3://sfquickstarts/data-engineering-with-snowpark-python/customer'\n",
    "\n",
    "\n",
    "Tutorial: https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_python/index.html?index=..%2F..index#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "import modin.pandas as spd  # Snowpark Pandas API\n",
    "import snowflake.snowpark.modin.plugin  # Snowpark pandas plugin for modin\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6659e70-913c-4978-8b2e-d154843328d7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "set_paths"
   },
   "outputs": [],
   "source": [
    "# Set Paths\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ef72-370b-41c4-a46f-bb3eb8f58f45",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "set_ddl",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Set DDL\n\n-- Warehouses\nCREATE OR REPLACE WAREHOUSE TEST_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\nUSE WAREHOUSE TEST_WH;\n\n-- Databases\nCREATE OR ALTER DATABASE data_engineering_pipelines_with_snowpark_python;\nUSE DATABASE data_engineering_pipelines_with_snowpark_python;\n\n-- Schemas\nCREATE OR ALTER SCHEMA EXTERNAL;\nCREATE OR ALTER SCHEMA RAW_POS;\nCREATE OR ALTER SCHEMA RAW_CUSTOMER;\nCREATE OR ALTER SCHEMA HARMONIZED;\nCREATE OR ALTER SCHEMA ANALYTICS;\n\n-- Stages\nUSE SCHEMA ANALYTICS;\nCREATE OR ALTER STAGE analytics_stage \n\tDIRECTORY = ( ENABLE = true ); \n\nUSE SCHEMA EXTERNAL;  \nCREATE OR ALTER STAGE frostbyte_raw_stage\n    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/';\n\n-- File formats\nUSE SCHEMA EXTERNAL;\nCREATE OR ALTER FILE FORMAT PARQUET_FORMAT \n    /*Note: When we loaded raw data from Parquet we can take advantage of Snowflake's schema detection feature \n    to create a table with the same schema as the Parquet files*/\n    TYPE = PARQUET\n    COMPRESSION = SNAPPY;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "language": "python",
    "name": "set_session"
   },
   "outputs": [],
   "source": [
    "# Create a snowpark session\n",
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session for troubleshooting and monitoring\n",
    "session.query_tag = {\n",
    "    \"origin\":\"sf_sit-is\", \n",
    "    \"name\":\"data_engineering_pipelines_with_snowpark_python\", \n",
    "    \"version\":{\"major\":1, \"minor\":0},\n",
    "    \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\", \"vignette\":\"snowpark_pandas\"}\n",
    "}\n",
    "\n",
    "# Set root\n",
    "root = Root(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f247e1-40d0-4003-92d5-3a19712254f1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "create_udf",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create UDF Functions: ANALYTICS\n",
    "\n",
    "USE SCHEMA ANALYTICS;\n",
    "\n",
    "-- INCH_TO_MILLIMETER_UDF\n",
    "CREATE OR ALTER FUNCTION INCH_TO_MILLIMETER_UDF(INCH NUMBER(35,4))\n",
    "RETURNS NUMBER(35,4)\n",
    "AS $$\n",
    "    inch * 25.4\n",
    "$$;\n",
    "\n",
    "/* Alternative Python Version\n",
    "def inch_to_millimeter(inch: float) -> float:\n",
    "    return inch * 25.4\n",
    "\n",
    "inch_to_millimeter_udf = session.udf.register(name=\"inch_to_millimeter_udf\", \n",
    "                                                func='inch_to_millimeter', \n",
    "                                                input_types=[FloatType()],\n",
    "                                                return_type=FloatType(),\n",
    "                                                replace=True, \n",
    "                                                is_permanent=True,\n",
    "                                                stage_location='@analytics_stage')\n",
    "*/\n",
    "\n",
    "-- FAHRENHEIT_TO_CELSIUS_UDF\n",
    "CREATE OR ALTER FUNCTION FAHRENHEIT_TO_CELSIUS_UDF(\"temp_f\" FLOAT)\n",
    "RETURNS FLOAT\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION=3.9 -- python version\n",
    "HANDLER = 'fahrenheit_to_celsius'  --function name\n",
    "AS $$\n",
    "def fahrenheit_to_celsius(temp_f: float) -> float:\n",
    "    return (float(temp_f) - 32) * (5/9)  # function definition\n",
    "$$;\n",
    "/* Alternative Python Version\n",
    "def fahrenheit_to_celsius(temp_f: float) -> float:\n",
    "    return (float(temp_f) - 32) * (5/9)\n",
    "\n",
    "fahrenheit_to_celsius_udf = session.udf.register(name=\"fahrenheit_to_celsius_udf\", \n",
    "                                                func='fahrenheit_to_celsius', \n",
    "                                                input_types=[FloatType()],\n",
    "                                                return_type=FloatType(),\n",
    "                                                replace=True, \n",
    "                                                is_permanent=True,\n",
    "                                                stage_location='@analytics_stage')\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "sql",
    "name": "test_INCH_TO_MILLIMETER_UDF",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Test INCH_TO_MILLIMETER_UDF (12 -> 304.8)\n",
    "SELECT data_engineering_pipelines_with_snowpark_python.ANALYTICS.INCH_TO_MILLIMETER_UDF(12);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc45020-cda9-46d0-afa4-7fbf0571971f",
   "metadata": {
    "language": "sql",
    "name": "test_FAHRENHEIT_TO_CELSIUS_UDF",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Test FAHRENHEIT_TO_CELSIUS_UDF (32 -> 0)\n",
    "SELECT data_engineering_pipelines_with_snowpark_python.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe721-66fc-4031-b3fe-6f195ee5baa7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "define_load_all_raw_tables"
   },
   "outputs": [],
   "source": "# Define load_all_raw_tables\n\ndef load_frostbyte_raw_table(session, tname=None, s3dir=None, year=None, schema=None):\n    \"\"\"This function extracts data from s3 and loads it into Snowflake\"\"\"\n    \n    # Set schema\n    session.use_schema(schema)\n\n    # Set parquet location path\n    if year is None:\n        pq_loc = f\"@external.frostbyte_raw_stage/{s3dir}/{tname}\"\n    else:\n        pq_loc = f\"@external.frostbyte_raw_stage/{s3dir}/{tname}/year={year}\"\n\n    # Extract the data from s3dir\n    # Note: When we loaded raw data from Parquet we can take advantage of Snowflake's schema detection\n    #  feature to create a table with the same schema as the Parquet files\n    df = session.read.option(\"compression\", \"snappy\").parquet(pq_loc)\n    \n    # Load data into raw table\n    df.copy_into_table(f\"{tname}\")\n\n\ndef load_all_frostbyte_raw_tables(session):\n    \"\"\"This function loops through specific tables in s3d to get the schema.table info and then calls load_raw_table\"\"\"\n    \n    pos_tables = ['country', 'franchise', 'location', 'menu', 'truck', 'order_header', 'order_detail']\n    customer_tables = ['customer_loyalty']\n    table_dict = {\n        \"pos\": {\"schema\": \"RAW_POS\", \"tables\": pos_tables},\n        \"customer\": {\"schema\": \"RAW_CUSTOMER\", \"tables\": customer_tables}\n    }\n    order_years = ['2019', '2020', '2021']  # Note: 2022 will be added later (see -- Add year 2022 to order_header_df)\n\n    # Temporarily increase warehouse size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n    for s3dir, data in table_dict.items():\n        tnames = data['tables']\n        schema = data['schema']\n        for tname in tnames:\n            print(f\"Loading {schema}.{tname}\")\n            if tname in ['order_header_df', 'order_detail_df']:\n                for year in order_years:\n                    print(f'\\tLoading year {year}') \n                    load_frostbyte_raw_table(session, tname=tname, s3dir=s3dir, year=year, schema=schema)\n            else:\n                load_frostbyte_raw_table(session, tname=tname, s3dir=s3dir, schema=schema)\n    \n    # Return warehouse to original size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL\").collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "language": "python",
    "name": "run_load_all_frostbyte_raw_tables"
   },
   "outputs": [],
   "source": [
    "# Run load_all_frostbyte_raw_tables\n",
    "load_all_frostbyte_raw_tables(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "load_weather"
   },
   "source": [
    "### Load Weather\n",
    "\n",
    "Connect to the \"Weather Source LLC: frostbyte\" feed from Weather Source in the Snowflake Data Marketplace by following these steps:\n",
    "\n",
    "    -> Snowsight Home Button\n",
    "         -> Marketplace\n",
    "             -> Search: \"Weather Source LLC: frostbyte\" (and click on tile in results)\n",
    "                 -> Click the blue \"Get\" button\n",
    "                     -> Under \"Options\", adjust the Database name to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n",
    "                        -> Grant to \"AccountAdmin\"\n",
    "    \n",
    "That's it... we don't have to do anything from here to keep this data updated.\n",
    "The provider will do that for us and data sharing means we are always seeing\n",
    "whatever they they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "language": "sql",
    "name": "view_weather",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View Weather data\n",
    "SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235cfa2-7694-4d77-bbf2-7d3c71c45ab1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_pos_flattened_v"
   },
   "outputs": [],
   "source": [
    "# Define Create \"HARMONIZED.POS_FLATTENED_V\" view and stream\n",
    "\n",
    "def create_pos_view(session):\n",
    "    # Set session schema\n",
    "    session.use_schema('HARMONIZED')\n",
    "\n",
    "    # Extract data to dataframes\n",
    "    truck_df = session.table(\"RAW_POS.TRUCK\")\n",
    "    franchise_df = session.table(\"RAW_POS.FRANCHISE\")\n",
    "    order_header_df = session.table(\"RAW_POS.ORDER_HEADER\")\n",
    "    location_df = session.table(\"RAW_POS.LOCATION\")\n",
    "    order_detail_df = session.table(\"RAW_POS.ORDER_DETAIL\")\n",
    "    menu_df = session.table(\"RAW_POS.MENU\")\n",
    "\n",
    "    # Modify dataframes\n",
    "    order_header_df = order_header_df.with_column(\"ORDER_TS_DATE\", F.to_date(F.col(\"ORDER_TS\")))  # Add Column\n",
    "    franchise_df = franchise_df.rename({\n",
    "        F.col(\"FIRST_NAME\"): \"FRANCHISEE_FIRST_NAME\",\n",
    "        F.col(\"LAST_NAME\"): \"FRANCHISEE_LAST_NAME\"\n",
    "    })\n",
    "\n",
    "    # Join dataframes\n",
    "    t_with_f = truck_df.join(franchise_df, truck_df['FRANCHISE_ID'] == franchise_df['FRANCHISE_ID'], rsuffix='_f')\n",
    "    oh_w_t_and_l = order_header_df.join(t_with_f, order_header_df['TRUCK_ID'] == t_with_f['TRUCK_ID'], rsuffix='_t') \\\n",
    "                                .join(location_df, order_header_df['LOCATION_ID'] == location_df['LOCATION_ID'], rsuffix='_l')\n",
    "    final_df = order_detail_df.join(oh_w_t_and_l, order_detail_df['ORDER_ID'] == oh_w_t_and_l['ORDER_ID'], rsuffix='_oh') \\\n",
    "                            .join(menu_df, order_detail_df['MENU_ITEM_ID'] == menu_df['MENU_ITEM_ID'], rsuffix='_m')\n",
    "    final_df = final_df.select(F.col(\"ORDER_ID\"), \\\n",
    "                            F.col(\"TRUCK_ID\"), \\\n",
    "                            F.col(\"ORDER_TS\"), \\\n",
    "                            F.col(\"ORDER_TS_DATE\"), \\\n",
    "                            F.col(\"ORDER_DETAIL_ID\"), \\\n",
    "                            F.col(\"LINE_NUMBER\"), \\\n",
    "                            F.col(\"TRUCK_BRAND_NAME\"), \\\n",
    "                            F.col(\"MENU_TYPE\"), \\\n",
    "                            F.col(\"PRIMARY_CITY\"), \\\n",
    "                            F.col(\"REGION\"), \\\n",
    "                            F.col(\"COUNTRY\"), \\\n",
    "                            F.col(\"FRANCHISE_FLAG\"), \\\n",
    "                            F.col(\"FRANCHISE_ID\"), \\\n",
    "                            F.col(\"FRANCHISEE_FIRST_NAME\"), \\\n",
    "                            F.col(\"FRANCHISEE_LAST_NAME\"), \\\n",
    "                            F.col(\"LOCATION_ID\"), \\\n",
    "                            F.col(\"MENU_ITEM_ID\"), \\\n",
    "                            F.col(\"MENU_ITEM_NAME\"), \\\n",
    "                            F.col(\"QUANTITY\"), \\\n",
    "                            F.col(\"UNIT_PRICE\"), \\\n",
    "                            F.col(\"PRICE\"), \\\n",
    "                            F.col(\"ORDER_AMOUNT\"), \\\n",
    "                            F.col(\"ORDER_TAX_AMOUNT\"), \\\n",
    "                            F.col(\"ORDER_DISCOUNT_AMOUNT\"), \\\n",
    "                            F.col(\"ORDER_TOTAL\"))\n",
    "        \n",
    "    # Create view from dataframe\n",
    "    final_df.create_or_replace_view('POS_FLATTENED_V')\n",
    "\n",
    "def create_pos_view_stream(session):\n",
    "    # Set session schema\n",
    "    session.use_schema('HARMONIZED')\n",
    "\n",
    "    # Create streaming view\n",
    "    session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n",
    "                        ON VIEW POS_FLATTENED_V \\\n",
    "                        SHOW_INITIAL_ROWS = TRUE').collect()\n",
    "\n",
    "def test_pos_view(session):\n",
    "    # Set session schema\n",
    "    session.use_schema('HARMONIZED')\n",
    "    session.table('POS_FLATTENED_V').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14352d-26ee-40a6-b1f5-0d94ffaa302a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "run_create_pos_flattened_v"
   },
   "outputs": [],
   "source": [
    "# Run Create \"HARMONIZED.POS_FLATTENED_V\" view and stream\n",
    "\n",
    "create_pos_view(session)\n",
    "create_pos_view_stream(session)\n",
    "test_pos_view(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75b295-bd82-4ed6-a565-259a56053c61",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "define_orders_upsert"
   },
   "outputs": [],
   "source": "orders_upsert_sp_script = \"\"\"\nimport time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\n\n# Define orders_upsert\ndef orders_upsert(session):\n    \n    # Create tables and stream if not exists\n    schema ='HARMONIZED'\n    table ='ORDERS'\n    exists = session.sql(f\"SELECT EXISTS ( \\\n                         SELECT * \\\n                         FROM INFORMATION_SCHEMA.TABLES \\\n                         WHERE TABLE_SCHEMA = '{schema}' \\\n                         AND TABLE_NAME = '{table}') \\\n                         AS TABLE_EXISTS\").collect()[0]['TABLE_EXISTS']\n    if not exists:\n        session.sql(\"CREATE TABLE HARMONIZED.ORDERS LIKE HARMONIZED.POS_FLATTENED_V\").collect()\n        session.sql(\"ALTER TABLE HARMONIZED.ORDERS ADD COLUMN META_UPDATED_AT TIMESTAMP\").collect()\n        session.sql(\"CREATE STREAM HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS\").collect()\n    \n    # Temporarily increase warehouse size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n    # Get data\n    source_df = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n    target_df = session.table('HARMONIZED.ORDERS')\n\n    # Upsert data\n    cols_to_update = {c: source_df[c] for c in source_df.schema.names if \"METADATA\" not in c}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    records = {**cols_to_update, **metadata_col_to_update}\n    target_df.merge(source_df, target_df['ORDER_DETAIL_ID'] == source_df['ORDER_DETAIL_ID'], \\\n                        [F.when_matched().update(records), F.when_not_matched().insert(records)])\n\n    # Return warehouse to original size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "sql",
    "name": "create_ORDERS_UPSERT_SP",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Create ORDERS_UPSERT_SP\nUSE SCHEMA HARMONIZED;\nCREATE OR REPLACE PROCEDURE ORDERS_UPSERT_SP()\n    RETURNS string\n    LANGUAGE PYTHON\n    RUNTIME_VERSION=3.9\n    PACKAGES=('snowflake-snowpark-python')\n    HANDLER = 'orders_upsert'  -- python function to execute from script\n    AS $$\n        {{orders_upsert_sp_script}} \n    $$;\n/*Alternative Python Version\nsession.sproc.register(name=\"ORDERS_UPSERT_SP\", \n                       func=orders_upsert,  \n                       packages=['snowflake-snowpark-python'])\n\n*/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "sql",
    "name": "call_ORDERS_UPSERT_SP",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Call ORDERS_UPSERT_SP\n",
    "CALL ORDERS_UPSERT_SP();\n",
    "/*Alternative Python Version\n",
    "session.call('ORDERS_UPSERT_SP')\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fa22c-f646-4ab8-aea4-6ac17e5b96e3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "define_daily_city_metrics_upsert"
   },
   "outputs": [],
   "source": "daily_city_metrics_upsert_sp_script = \"\"\"\n\nimport time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n# Define daily_city_metrics_upsert\ndef daily_city_metrics_upsert(session):\n\n    # Create table if not exists\n    schema ='ANALYTICS'\n    table ='DAILY_CITY_METRICS'\n    exists = session.sql(f\"SELECT EXISTS ( \\\n                         SELECT * \\\n                         FROM INFORMATION_SCHEMA.TABLES \\\n                         WHERE TABLE_SCHEMA = '{schema}' \\\n                         AND TABLE_NAME = '{table}') \\\n                         AS TABLE_EXISTS\").collect()[0]['TABLE_EXISTS']\n    if not exists:\n        # In this step we are explicitly defining the schema in DataFrame syntax and using that to create the table.\n        shared_columns= [\n            T.StructField(\"DATE\", T.DateType()),\n            T.StructField(\"CITY_NAME\", T.StringType()),\n            T.StructField(\"COUNTRY_DESC\", T.StringType()),\n            T.StructField(\"DAILY_SALES\", T.StringType()),\n            T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n            T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n            T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n            T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n            T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n        ]\n        daily_city_metrics_columns = [\n            *shared_columns, \n            T.StructField(\"META_UPDATED_AT\", T.TimestampType())\n        ]\n        daily_city_metrics_schema = T.StructType(daily_city_metrics_columns)\n        daily_city_metrics_table = 'ANALYTICS.DAILY_CITY_METRICS'\n        session.create_dataframe([[None]*len(daily_city_metrics_schema.names)], schema=daily_city_metrics_schema) \\\n                .na.drop() \\\n                .write.mode('overwrite').save_as_table(daily_city_metrics_table)\n\n    # Temporarily increase warehouse size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n \n    \n    # Get data\n    orders_stream_dates_df = session.table('HARMONIZED.ORDERS_STREAM').select(F.col(\"ORDER_TS_DATE\").alias(\"DATE\")).distinct()\n    orders_stream_dates_df.limit(5).show()\n    orders_df = session.table(\"HARMONIZED.ORDERS_STREAM\")\\\n                        .group_by(\\\n                            F.col('ORDER_TS_DATE'), \\\n                            F.col('PRIMARY_CITY'), \\\n                            F.col('COUNTRY')) \\\n                        .agg(\\\n                            F.sum(F.col(\"PRICE\")).as_(\"price_nulls\")) \\\n                        .with_column(\\\n                            \"DAILY_SALES\", \\\n                            F.call_builtin(\"ZEROIFNULL\", \\\n                            F.col(\"price_nulls\"))) \\\n                        .select(\\\n                            F.col('ORDER_TS_DATE').alias(\"DATE\"), \\\n                            F.col(\"PRIMARY_CITY\").alias(\"CITY_NAME\"), \\\n                            F.col(\"COUNTRY\").alias(\"COUNTRY_DESC\"), \\\n                            F.col(\"DAILY_SALES\"))\n    weather_pc_df = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES\")\n    countries_df = session.table(\"RAW_POS.COUNTRY\")\n    weather_df = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n    weather_df = weather_df.join(weather_pc_df, on=(weather_df['POSTAL_CODE'] == weather_pc_df['POSTAL_CODE']) & (weather_df['COUNTRY'] == weather_pc_df['COUNTRY']), rsuffix='_pc')\n    weather_df = weather_df.join(countries_df,on=(weather_df['COUNTRY'] == countries_df['ISO_COUNTRY']) & (weather_df['CITY_NAME'] == countries_df['CITY']), rsuffix='_c')\n    weather_df = weather_df.join(orders_stream_dates_df, on=weather_df['DATE_VALID_STD'] == orders_stream_dates_df['DATE']) \\\n                    .group_by(\\\n                        F.col('DATE_VALID_STD'), \\\n                        F.col('CITY_NAME'), \\\n                        F.col('COUNTRY_C')) \\\n                    .agg(\\\n                        F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                        F.avg(F.call_udf(\"ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF\", F.col(\"AVG_TEMPERATURE_AIR_2M_F\"))).alias(\"AVG_TEMPERATURE_C\"), \\\n                        F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                        F.avg(F.call_udf(\"ANALYTICS.INCH_TO_MILLIMETER_UDF\", F.col(\"TOT_PRECIPITATION_IN\"))).alias(\"AVG_PRECIPITATION_MM\"), \\\n                        F.max(F.col(\"MAX_WIND_SPEED_100M_MPH\")).alias(\"MAX_WIND_SPEED_100M_MPH\")) \\\n                    .select(\\\n                        F.col(\"DATE_VALID_STD\").alias(\"DATE\"), \\\n                        F.col(\"CITY_NAME\"), \\\n                        F.col(\"COUNTRY_C\").alias(\"COUNTRY_DESC\"), \\\n                        F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                        F.round(F.col(\"AVG_TEMPERATURE_C\"), 2).alias(\"AVG_TEMPERATURE_CELSIUS\"), \\\n                        F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                        F.round(F.col(\"AVG_PRECIPITATION_MM\"), 2).alias(\"AVG_PRECIPITATION_MILLIMETERS\"), \\\n                        F.col(\"MAX_WIND_SPEED_100M_MPH\"))\n    # Stage data\n    daily_city_metrics_stg_df = orders_df.join(weather_df, on=(orders_df['DATE'] == weather_df['DATE']) & (orders_df['CITY_NAME'] == weather_df['CITY_NAME']) & (orders_df['COUNTRY_DESC'] == weather_df['COUNTRY_DESC']), how='left', rsuffix='_w') \\\n                                   .select(\\\n                                        \"DATE\", \\\n                                        \"CITY_NAME\", \\\n                                        \"COUNTRY_DESC\", \\\n                                        \"DAILY_SALES\", \\\n                                        \"AVG_TEMPERATURE_FAHRENHEIT\", \\\n                                        \"AVG_TEMPERATURE_CELSIUS\", \\\n                                        \"AVG_PRECIPITATION_INCHES\",\\\n                                        \"AVG_PRECIPITATION_MILLIMETERS\", \\\n                                        \"MAX_WIND_SPEED_100M_MPH\")\n\n    # Upsert data\n    cols_to_update = {c: daily_city_metrics_stg_df[c] for c in daily_city_metrics_stg_df.schema.names}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    records = {**cols_to_update, **metadata_col_to_update}\n    daily_city_metrics_df = session.table('ANALYTICS.DAILY_CITY_METRICS')\n    daily_city_metrics_df.merge(daily_city_metrics_stg_df, \\\n                                  (daily_city_metrics_df['DATE'] == daily_city_metrics_stg_df['DATE']) \\\n                                & (daily_city_metrics_df['CITY_NAME'] == daily_city_metrics_stg_df['CITY_NAME']) \\\n                                & (daily_city_metrics_df['COUNTRY_DESC'] == daily_city_metrics_stg_df['COUNTRY_DESC']), \\\n                                [F.when_matched().update(records), F.when_not_matched().insert(records)])\n\n    # Return warehouse to original size\n    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b166f6-97d8-4d08-8c41-f49c0aae1aec",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "create_DAILY_CITY_METRICS_UPSERT_SP",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Create DAILY_CITY_METRICS_UPSERT_SP\nUSE SCHEMA HARMONIZED;\nCREATE OR REPLACE PROCEDURE DAILY_CITY_METRICS_UPSERT_SP()\n    RETURNS string\n    LANGUAGE PYTHON\n    RUNTIME_VERSION=3.9\n    PACKAGES=('snowflake-snowpark-python')\n    HANDLER = 'daily_city_metrics_upsert'\n    AS $$\n        {{daily_city_metrics_upsert_sp_script}}\n    $$;\n/*Alternative Python Version\nsession.sproc.register(name=\"DAILY_CITY_METRICS_UPSERT_SP\", \n                       func=daily_city_metrics_upsert,  \n                       packages=['snowflake-snowpark-python'])\n\n*/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5289e-5197-4bda-ac42-551c609787b5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "call_DAILY_CITY_METRICS_UPDATE_SP",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Call DAILY_CITY_METRICS_UPDATE_SP\nCALL DAILY_CITY_METRICS_UPSERT_SP();\n/*Alternative Python Version\nsession.call('DAILY_CITY_METRICS_UPSERT_SP')\n*/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868473c7-aea4-430a-be35-7f710fc34729",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "tasks",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Tasks\n\n-- ----------------------------------------------------------------------------\n-- Step #1: Create the tasks that trigger based on new data pushes\n-- ----------------------------------------------------------------------------\n\n-- ORDERS_UPSERT_TASK\nUSE SCHEMA HARMONIZED;\nCREATE OR ALTER TASK ORDERS_UPSERT_TASK\nWAREHOUSE = TEST_WH\nWHEN\n  SYSTEM$STREAM_HAS_DATA('POS_FLATTENED_V_STREAM')\nAS \nCALL HARMONIZED.ORDERS_UPSERT_SP();\n\n-- DAILY_CITY_METRICS_UPSERT_TASK\nUSE SCHEMA HARMONIZED;\nCREATE OR ALTER TASK DAILY_CITY_METRICS_UPSERT_TASK\nWAREHOUSE = TEST_WH\nAFTER ORDERS_UPSERT_TASK\nWHEN\n  SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\nAS\nCALL HARMONIZED.DAILY_CITY_METRICS_UPSERT_SP();\n\n-- ----------------------------------------------------------------------------\n-- Step #2: Execute the tasks\n-- ----------------------------------------------------------------------------\n\nALTER TASK DAILY_CITY_METRICS_UPSERT_TASK RESUME;\n\nEXECUTE TASK ORDERS_UPSERT_TASK;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8084fdd-ad00-4d02-b762-bbd076d17bd3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "add_to_order_header",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": "-- Add year 2022 to order_header_df\n\n-- Set schema\nUSE SCHEMA RAW_POS;\n\n-- Temporarily increase warehouse size\nALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE;\n\n-- Set parquet location path\nLS @external.frostbyte_raw_stage/pos/order_header/year=2022;\n\n--\nCOPY INTO order_header \nFROM @external.frostbyte_raw_stage/pos/order_header/year=2022\nFILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\nMATCH_BY_COLUMN_NAME = CASE_SENSITIVE;\n\n--\nCOPY INTO order_detail \nFROM @external.frostbyte_raw_stage/pos/order_detail/year=2022\nFILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\nMATCH_BY_COLUMN_NAME = CASE_SENSITIVE;\n\n-- Return warehouse to original size\nALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL;\n\n/* Alternative Python Version\nschema = \ntables = ['order_header', 'order_header']\nyear = 2022\n\n# Set schema\nsession.use_schema(schema)\n\n# Temporarily increase warehouse size\nsession.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\nfor tname in tables:\n    # Set parquet location path\n    location = f\"@external.frostbyte_raw_stage/pos/{tname}/year={year}\"\n    \n    # Extract the data from s3dir\n    \"\"\"Note: When we loaded raw data from Parquet we can take advantage of Snowflake's schema detection feature \n    to create a table with the same schema as the Parquet files\"\"\"\n    df = session.read.option(\"compression\", \"snappy\").parquet(location)\n\n    # Load data into raw table\n    df.copy_into_table(f\"{tname}\")\n\n# Return warehouse to original size\nsession.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n*/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "snowflake-sql"
    },
    "name": "teardown",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Teardown\n",
    "\n",
    "DROP DATABASE data_engineering_pipelines_with_snowpark_python;\n",
    "\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  }
 ]
}